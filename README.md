# Домашнее задание к занятию "3.8. Компьютерные сети, лекция 3"

1. Почему висит InActConn. 
Почитал документ https://docs.huihoo.com/hpc-cluster/linux-virtual-server/HOWTO/LVS-HOWTO.ipvsadm.html 
В  режиме direct routing часть tcp сессии идет через директор, а часть через конечный сервер. В нашем примере NGINX. 
Директор не понимает состояния TCP сессии поэтому какое-то время пытается держать сессию в состоянии InActConn, время удержания сессии можно задавать в настройках. В NAT режиме директор бы видел всю сессию целиком.

2. Создана тестовая среда из 5 машин: 1 клиент client (192.168.10.10), 2 директора lb01 и lb02 (192.168.10.11 и 12), 2 NGINX сервера nginx01 и nginx02 (192.168.10.13 и 14).
vagrant файл src\vagrant
Конфигурация директоров в папках src\lb01 и src\lb02. Приведены конфигурации для настройки ipvsmadm и keepalived.
Конфигурации NGINX серверов в папках src\nginx01 и src\nginx02. Приведены конфигурации для настройки IP адресов для работы ipvsadm и необходимые для корректной работы параметры sysctl.
\
Решение проблемы проверки NGINX сервера. Вижу 2 простых решения: или с директоров по крону проверять доступность сервиса и включать или исключать нужный NGINX сервер из ipvs, или на NGINX серверах поднять дополнительные VIP адреса с проверками доступности сервиса NGINX и завязать ipvs на них. VIP адреса NGINX серверов, по неактивности, будут переезжать на другие NGINX сервера в VRRP кластере. 
Пошел по пути keepalived. Конфигурация лежит src\nginx01\etc\keepalived и src\nginx02\etc\keepalived

3. Самое простое - повесить 2 VIP адреса на эти машины. Но нет никакой гарантии, что при выключении случайной машины эти 2 IP адреса не окажутся на одной машине. Что приведет к перегрузке интерфейса. 
Поэтому, лучше всего сделать 3 VIP адреса по одному накаждую машину. Тогда при отключении любой машины проучится гарантировать, что на оставшихся машинах будет минимум по одному IP адресу.


# Работа над ошибками

1. Почитал про Virtual Server Definitions, было бы неплохо если про это рассказывали в теме. Был уверен что keepalived только с VRRP может работать. То что лучше всего проверять доступность на самих балансерах тоже понятно и логично. Но тема все-таки про балансировку ) 
2. Исходил из минимально необходимого количества IP адресов. Для директоров думаю лучше всего интерфейсы с VIP адресами и с адресами для общения с бэкэндом разделять. Мониторинг не должен давать какую-либо существенную нагрузку, по VIP адресам можно мониторить доступность сервиса, по внутренним адресам прочие метрики.  
   Можно увеличить количество VIP интерйесов до 4. Тогда можно настроить все так, что после выпадения любой ноды гарантировано трафик делился бы пополам (по 2 VIP на ноду). Но при такой конфигурации одна нода была бы всегда нагружена на 1/2 от всего трафика.  
   Можно взять 6 адресов, тогда нагрузка при нормальной работе рабивается поровну. И в случе выхода одной ноды из строя, можно равномерно распределить VIP адреса по другим нодам балансера. Но мне такое решение не нравится именно в части потребленияи IP адресов, думаю на практике я бы его не применял. Возможно бы увеличивали количество узлов балнсера или переходили уже на 10 Гбит/сек.
